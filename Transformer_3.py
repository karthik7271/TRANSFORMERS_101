# -*- coding: utf-8 -*-
"""Transformer_3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1B_a9Bvrs4RWfVuCY_Fk9qACC0PIXnKiQ
"""

import transformers

from transformers import AutoTokenizer,AutoModelForCausalLM
import torch

device ="cuda" if torch.cuda.is_available() else "cpu"

device

model_name="gpt2-large"
tokenizer=AutoTokenizer.from_pretrained(model_name)

model=AutoModelForCausalLM.from_pretrained(model_name).to(device)

input_text="i am really impressed"
max_length=128
input_ids=tokenizer(input_text,return_tensors="pt")["input_ids"].to(device)

input_ids

output=model.generate(input_ids,
                      max_length=max_length,
                      do_sample=True,
                      top_p=0.9)

tokenizer.decode(output[0])





























